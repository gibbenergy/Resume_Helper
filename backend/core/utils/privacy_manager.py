"""
Privacy Manager - Protect personal information from being sent to LLM providers

This module provides data sanitization and personal information management
to ensure user privacy while maintaining AI functionality.
"""

import copy
import logging
from typing import Dict, Any, Tuple, List, Optional

logger = logging.getLogger(__name__)

class PrivacyManager:
    """Manages privacy protection for resume data sent to LLM providers."""
    
    SENSITIVE_PERSONAL_FIELDS = [
        'name_prefix', 'full_name', 'name', 'email', 'phone', 'current_address', 
        'address', 'location', 'citizenship', 'linkedin_url', 
        'github_url', 'portfolio_url'
    ]
    

    
    @classmethod
    def sanitize_resume_data(cls, resume_data: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """
        Sanitize resume data by completely removing personal information for LLM processing.
        
        This follows the user's privacy-first approach:
        1. Remove ALL personal info before sending to LLM
        2. LLM processes only professional content 
        3. Personal info gets added back during document generation
        
        Args:
            resume_data: Complete resume data dictionary
            
        Returns:
            Tuple of (sanitized_data, personal_info)
            - sanitized_data: Professional content only - SAFE for LLM providers
            - personal_info: Complete personal data for document generation
        """
        if not isinstance(resume_data, dict):
            logger.warning("Invalid resume data format for sanitization")
            return {}, {}
        
        sanitized_data = copy.deepcopy(resume_data)
        personal_info = {}
        
        if 'personal_info' in sanitized_data:
            personal_section = sanitized_data.pop('personal_info', {})
            personal_info['personal_info'] = personal_section
            
            safe_professional = {}
            if 'summary' in personal_section:
                safe_professional['summary'] = personal_section['summary']
            if 'objective' in personal_section:
                safe_professional['objective'] = personal_section['objective']
            
            if safe_professional:
                sanitized_data['professional_summary'] = safe_professional
        
        for field in cls.SENSITIVE_PERSONAL_FIELDS:
            if field in sanitized_data:
                personal_info[field] = sanitized_data.pop(field, None)
        
        cls._sanitize_work_experience(sanitized_data, personal_info)
        
        removed_fields = list(personal_info.keys())
        if removed_fields:
            logger.info(f"ðŸ›¡ï¸ PRIVACY PROTECTION: Removed {len(removed_fields)} personal fields - LLM will receive ONLY professional content")
            logger.debug(f"ðŸ›¡ï¸ Removed personal fields: {removed_fields}")
        else:
            logger.info("ðŸ›¡ï¸ PRIVACY PROTECTION: No personal fields found to remove")
        
        is_safe, violations = cls.validate_sanitized_data(sanitized_data)
        if not is_safe:
            logger.error(f"ðŸš¨ PRIVACY BREACH DETECTED! Personal data still present: {violations}")
            for violation in violations:
                if '.' in violation:
                    section, field = violation.split('.', 1)
                    if section in sanitized_data and isinstance(sanitized_data[section], dict):
                        sanitized_data[section].pop(field, None)
                else:
                    sanitized_data.pop(violation, None)
            logger.info("ðŸ›¡ï¸ Emergency cleanup completed - personal data removed")
        
        return sanitized_data, personal_info
    
    @classmethod
    def _sanitize_work_experience(cls, sanitized_data: Dict[str, Any], personal_info: Dict[str, Any]) -> None:
        """Sanitize work experience to remove personally identifiable information."""
        experience_fields = ['experience', 'work_experience', 'experiences']
        
        for field in experience_fields:
            if field in sanitized_data and isinstance(sanitized_data[field], list):
                for i, exp in enumerate(sanitized_data[field]):
                    if isinstance(exp, dict):
                        if 'manager' in exp:
                            personal_info.setdefault('work_contacts', []).append({
                                'company': exp.get('company', ''),
                                'manager': exp.pop('manager', '')
                            })
    
    @classmethod
    def merge_personal_info_for_documents(cls, professional_content: Dict[str, Any], personal_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        Merge personal information back with professional content for document generation.
        
        This is the final step in the privacy-first workflow:
        1. LLM processed only professional content (no personal info)
        2. Now we merge personal info back for PDF/document generation
        
        Args:
            professional_content: LLM-processed professional content (sanitized)
            personal_info: Personal information extracted earlier
            
        Returns:
            Complete resume data ready for document generation
        """
        complete_resume = copy.deepcopy(professional_content)
        
        logger.debug(f"ðŸ” MERGE: professional_content keys = {list(professional_content.keys())}")
        logger.debug(f"ðŸ” MERGE: personal_info keys = {list(personal_info.keys())}")
        
        # Preserve AI-tailored summary if it exists in professional_content
        tailored_summary = None
        if 'summary' in professional_content:
            tailored_summary = professional_content['summary']
            logger.info(f"ðŸ” Found AI-tailored summary in professional_content (length: {len(tailored_summary) if isinstance(tailored_summary, str) else 'N/A'})")
        else:
            logger.debug(f"ðŸ” No 'summary' key in professional_content")
        
        if 'personal_info' in personal_info:
            complete_resume['personal_info'] = personal_info['personal_info']
            original_summary_in_personal = complete_resume.get('personal_info', {}).get('summary', '')
            logger.debug(f"ðŸ” Original summary in personal_info (length: {len(original_summary_in_personal) if original_summary_in_personal else 0})")
            
            # Update personal_info.summary with tailored version if available
            if tailored_summary and isinstance(tailored_summary, str):
                complete_resume['personal_info']['summary'] = tailored_summary
                logger.info(f"ðŸ” Preserved AI-tailored summary in personal_info.summary (replaced original)")
            elif 'summary' in complete_resume.get('personal_info', {}):
                logger.debug(f"ðŸ” Using original summary from personal_info (no tailored version found)")
            else:
                logger.warning(f"ðŸ” No summary found in personal_info after merge")
        
        for field in cls.SENSITIVE_PERSONAL_FIELDS:
            if field in personal_info:
                complete_resume[field] = personal_info[field]
        
        if 'work_contacts' in personal_info:
            logger.debug(f"ðŸ” Work contacts available for restoration: {len(personal_info['work_contacts'])}")
        
        logger.info(f"ðŸ” DOCUMENT GENERATION: Merged personal info back with professional content")
        logger.debug(f"ðŸ” Personal fields restored: {[k for k in personal_info.keys() if k != 'work_contacts']}")
        
        return complete_resume
    

    

    
    @classmethod
    def validate_sanitized_data(cls, sanitized_data: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """
        Validate that sanitized data doesn't contain personal information.
        
        Args:
            sanitized_data: Data that should be safe for LLM processing
            
        Returns:
            Tuple of (is_safe, violations_found)
        """
        violations = []
        
        if 'personal_info' in sanitized_data:
            personal_section = sanitized_data['personal_info']
            for field in cls.SENSITIVE_PERSONAL_FIELDS:
                if field in personal_section:
                    violations.append(f"personal_info.{field}")
        
        for field in cls.SENSITIVE_PERSONAL_FIELDS:
            if field in sanitized_data:
                violations.append(field)
        
        is_safe = len(violations) == 0
        
        if not is_safe:
            logger.error(f"ðŸš¨ Privacy violation detected! Sensitive fields found in sanitized data: {violations}")
        else:
            logger.info("âœ… Privacy validation passed - no sensitive data detected")
        
        return is_safe, violations


class PrivacyAwareWorkflowMixin:
    """Mixin class to add privacy-aware methods to workflow classes."""
    
    def _prepare_safe_resume_data(self, resume_data: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """
        Prepare resume data for safe LLM processing.
        
        Returns:
            Tuple of (sanitized_data, personal_info)
        """
        sanitized_data, personal_info = PrivacyManager.sanitize_resume_data(resume_data)
        
        is_safe, violations = PrivacyManager.validate_sanitized_data(sanitized_data)
        if not is_safe:
            logger.error(f"ðŸš¨ PRIVACY BREACH PREVENTED! Violations: {violations}")
            for violation in violations:
                if '.' in violation:
                    section, field = violation.split('.', 1)
                    if section in sanitized_data and isinstance(sanitized_data[section], dict):
                        sanitized_data[section].pop(field, None)
                else:
                    sanitized_data.pop(violation, None)
        
        return sanitized_data, personal_info  
